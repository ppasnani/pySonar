#!/usr/bin/python -tt

import pyaudio
import wave
from pylab import *
import time
import Queue
import winsound
import threading
import numpy as np
import matplotlib.animation as animation
import matplotlib.pyplot as plt
import scipy
import scipy.fftpack


ChirpFile = "Sound Files/plain_chirp.wav"
transmitChirp = wave.open(ChirpFile, 'rb')
ChirpPeriod = 1

Chunk = 1024
Format = pyaudio.paInt16
SampWidth = transmitChirp.getsampwidth()
Channels = transmitChirp.getnchannels()
Rate = transmitChirp.getframerate()
#Time in seconds to display at one time on the waveform display
WindowTime = 1 

#Create a synchronized queue that can only hold one list
recordQueue = Queue.Queue(Chunk)

#Callback functions for playing back and recording audio in
#non-blocking mode. See reference 4
def playcallback(in_data, frame_count, time_info, status):
    out_data = transmitChirp.readframes(frame_count)
    return (out_data , pyaudio.paContinue)

def recordcallback(in_data, frame_count, time_info, status):
    recordQueue.put(fromstring(in_data, 'Int16'))
    return (in_data, pyaudio.paContinue)   

#Require: Requires an exisitng pyAudio object which has set
#       up the port audio system
#Ensures: Plays the ChirpFile wave file over the speakers
#       if the speakers are available

#Note: look into threading this. I think the print delay is the only thing
#causing this to play properly
def chirp(p):
    while True:
        #Returns a tuple (nchannels, sampwidth, framerate, nframes, comptype, compname)
        audioParams = transmitChirp.getparams()
        playstream = p.open(format=p.get_format_from_width(audioParams[1]),
                        channels=audioParams[0],
                        rate=audioParams[2],
                        input=False,
                        output=True,
                        stream_callback=playcallback)
        audioTime = round(float(audioParams[-3]) / audioParams[2], 6)
        #print 'AudioTime:', audioTime
        #Keep just playing the chirp and showing the data
        playstream.start_stream()
        #Temporary delay. Use some function of chirp length or
        #travel time for audio to determine this after threading function
        time.sleep(ChirpPeriod)
        playstream.stop_stream()
        playstream.close()
        transmitChirp.setpos(0)

#Requires: A start and stop number and the number of points n
#Ensures: Returns a list of floats of size n such that the first
#       item is the start number, the last item is the stop number
#Code from [5]
def myfrange(start, stop, n):
    L = [0.0] * n
    nm1 = n - 1
    nm1inv = 1.0 / nm1
    for i in range(n):
        L[i] = nm1inv * (start*(nm1 - i) + stop*i)
    return L

#Using [1] as reference
def showtimespectrum(audio):
    #subplot(211)
    #plot(audio)
    title('Spectrogram of recorded audio')
    #subplot(212)    
    audioTime = float(len(audio)) / Rate
        
    Pxx, freqs, bins, im = specgram(audio, Fs = Rate, scale_by_freq=True, sides='default')
    
    xlim([0, audioTime])
    pause(0.5)
    #show()

def plotspectrum(i, line, ax, rStream):
    if rStream.is_active():
        recordData = []        
        for x in range(recordQueue.qsize()):
            recordData.extend(recordQueue.get())
            recordQueue.task_done()
            
            FFT = abs(scipy.fft(recordData))
            timestep = 1 / float(Rate)
            freqs = scipy.fftpack.fftfreq(len(recordData), timestep)
            line.set_xdata(freqs)
            line.set_ydata(20*scipy.log10(FFT))
            #ax.autoscale_view()
            ax.set_ylim(0, 200)
            ax.set_xlim(-Rate/2, Rate/2)
            #print 'DUUUDE. I am FFTING', freqs
    return line,

def showspectrum(rStream):
    #Pxx, freqs, bins, im = specgram([], Fs = Rate, scale_by_freq = True, sides = 'default')
    fig, ax = plt.subplots()
    line, = ax.plot([],[])
    aniF = animation.FuncAnimation(fig, plotspectrum, interval = 25, blit = True,
                                   fargs = (line, ax, rStream))
    ax.clear()
    show()

def plotwave(i, line, ax, rStream):

    if rStream.is_active():
        recordData = []
        for x in range(recordQueue.qsize()):
            recordData.extend(recordQueue.get())
            recordQueue.task_done()

        old_x_data = line.get_xdata()
        old_y_data = line.get_ydata()
        old_sample_len = len(old_x_data)
        new_sample_len = len(recordData)
        #Length in seconds of old and new data                        
        old_len = old_sample_len / float(Rate)
        new_len = new_sample_len / float(Rate)
        
        #x-axis in seconds
        xmin, xmax = ax.get_xlim()
        
        #If old data is atleast the length of waterfall time then just
        #move the window forward. Otherwise keep adding data until atleast
        #window time
        if old_len >= WindowTime:
            ax.set_xlim(xmin+new_len, xmax+new_len)
            line.set_xdata(myfrange(xmin+new_len, xmax+new_len, old_sample_len))            
            line.set_ydata(append(old_y_data[new_sample_len:], recordData))
        else:
            ax.set_xlim(xmin, xmax+new_len)
            #print 'Build window'
            line.set_xdata(myfrange(xmin, xmax+new_len, old_sample_len + new_sample_len))
            line.set_ydata(append(old_y_data, recordData))
                  
    return line, 
    

def showwave(rStream):    
    #Pxx, freqs, t, fig = specgram([])
    #fig, ax = plt.specgram([1])
    fig, ax = plt.subplots()
    line, = ax.plot([], [])
    #Y Limit based on audio bit depth of 16 for now
    #
    ax.set_ylim(-1000, 1000)
    print "Brah"
    ani = animation.FuncAnimation(fig, plotwave, interval = 5, blit = True,
                                  fargs=(line, ax, rStream))
    ax.clear()
    plt.show()


def main():
    #Instantiate pyAudio
    p = pyaudio.PyAudio()
    #Open stream to record audio data from the mike
    recordstream = p.open(format=p.get_format_from_width(SampWidth),
                          channels=Channels,
                          rate=Rate,
                          input=True,
                          output=False,
                          stream_callback=recordcallback)
    #Create a thread to chirp continuously
    t1 = threading.Thread(target=chirp, args=(p,))
    t1.setDaemon(True)
    t1.start()
    #Create a "thread" to record continuosly
    recordstream.start_stream()    
    
    #Comment out one of the below two lines to show either the audio
    #waveform or the FFT spectrum
    #Keep both commented out if using the following for loop to show
    #timespectrum
    #showwave(recordstream)
    #showspectrum(recordstream)

    #Old code to display fft spectrum versus time
    #Buggy and slow as hell but worth keeping
    while recordstream.is_active():                
        #Get the recorded data from the synchronized priority queue
        recordData = []
        for i in range(recordQueue.qsize()):            
            recordData.extend(recordQueue.get())
            recordQueue.task_done()            
        #Display it
        showtimespectrum(recordData)
        
    #Wait for the threads to end and cleanup
    t1.join()
        
    recordQueue.join()
    recordstream.stop_stream()
    recordstream.close()
    
    p.terminate()

if __name__ == '__main__':
    main()
